#!/usr/bin/env bash
set -eu

error() { echo >&2 "$@"; false; }

# use ./giraph-debug in place of hadoop jar when using graft
case $debugConfig in
    none)
        cmd=(
        hadoop jar
        )
        ;;
    *)
        cmd=(
        ./giraph-debug
        $(cat args-giraph-debug 2>/dev/null || true)
        )
esac

# make sure input and output is possible
inputPath=graphs/$graph
outputPath=outputs/$algo/$graph/$_3X_RUN/
outputPathParent=$(dirname "$outputPath")
hadoop fs -test -e $inputPath ||
    error "$inputPath: Graph not found on HDFS"
hadoop fs -test -e $outputPathParent ||
    hadoop fs -mkdir $outputPathParent

# Java VM options for each worker
jvmOpts=(
-XX:+UseParallelGC
-XX:+PrintGC
-XX:+PrintGCTimeStamps
-XX:+PrintGCDetails
)

# construct list of command-line arguments for GiraphRunner
giraphRunnerArgs=(
$(cat args-algo)
"$@"

-vip $inputPath
-vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat
-op $outputPath

-w $numWorkers

# some Giraph/Hadoop configurations to make the runs smoother
-ca mapred.map.child.java.opts="${jvmOpts[*]}"
#-ca mapreduce.job.counters.limit=100000
-ca giraph.useSuperstepCounters=false
)

# make sure the job we launch below is killed when interrupted
cleanup() {
    jobid=$(set +x; grep <../stderr "[m]apred.JobClient: Running job: job_.*" | sed 's/.* Running job: //')
    [ -z "$jobid" ] || hadoop job -kill $jobid
    exit 255
}
trap cleanup INT QUIT TERM

# launch a job
set -x
"${cmd[@]}" target/giraph-debugger-*-jar-with-dependencies.jar \
    org.apache.giraph.GiraphRunner "${giraphRunnerArgs[@]}"
